{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hezbollah\n",
      "vocab len:52509\n",
      "Training: 380 pos, 1230 neg, Testing: 678 post, 0 neg\n",
      "group hezbollah itr 3, 678/678, Accuracy:1.000000\n",
      "0 class (0/0/0): precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class (678/678/678): precision:1.000000, recall:1.000000, fmeasure:1.000000\n",
      "group hezbollah: , Accuracy:1.000000\n",
      "0 class: precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class: precision:1.000000, recall:1.000000, fmeasure:1.000000\n",
      "Average:\n",
      "Accuracy:1.000000\n",
      "0 class: precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class: precision:1.000000, recall:1.000000, fmeasure:1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Input, Merge, Convolution1D, MaxPooling1D\n",
    "import ast\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from __future__ import print_function\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from functools import reduce\n",
    "import os\n",
    "from os.path import basename,isfile\n",
    "import csv \n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "sequence_length = 1000 #1980\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "#\n",
    "# Model Variations. See Kim Yoon’s Convolutional Neural Networks for \n",
    "# Sentence Classification, Section 3 for detail.\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 8\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 128\n",
    "#dropout_prob = (0.1, 0.75)\n",
    "dropout_prob = (0.1, 0.35)\n",
    "hidden_dims = 256 \n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 40\n",
    "#val_split = 0.33\n",
    "\n",
    "done=['sermons', 'houthis']\n",
    "#done=['ACLU','AEU','alqaeda','AndrewMurray','aqis','asl','ast',\n",
    "#        'Bahai','boko-haram','DorothyDay','Ghandi','YV','isis',\n",
    "#        'JohnPiper','LiberalJudaism','MalcolmX','MehrBaba','NaumanKhan',\n",
    "#        'nusra-front','PastorAnderson','PeterGomes','Rabbinic',\n",
    "#        'Schizophrenia','SeaShepherds','Shepherd','Stalin','taliban',\n",
    "#        'Unabomber','Unitarian','WBC','hamza-yusuf','yasir-qadhi',\n",
    "#        'zaki-naik','suhaib-webb','kkk','white-supremacis']\n",
    "TEXT_DATA_DIR='arabic-docs'\n",
    "accuracy=[]\n",
    "pre_0=[]\n",
    "rec_0=[]\n",
    "f1_0=[]\n",
    "pre_1=[]\n",
    "rec_1=[]\n",
    "f1_1=[]\n",
    "model_variation = 'CNN-rand'  #  CNN-rand | CNN-google | CNN-RPC\n",
    "#print('Model variation is %s' % model_variation)\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "#\n",
    "# Load data\n",
    "#print(\"Loading data...\")\n",
    " \n",
    "\n",
    "#load labels\n",
    "filePath='arabic-groups-labels.txt'\n",
    "labels={}\n",
    "with open(filePath,'r') as intputFile:\n",
    "        reader=csv.reader(intputFile,delimiter=',')\n",
    "        for fname,y in reader:\n",
    "            labels[fname]=int(y)\n",
    "# load all data\n",
    "all_sentences = []  # list of text articles\n",
    "all_labels_index = {}  # dictionary mapping label name to numeric id\n",
    "all_labels = []  # list of label ids\n",
    "\n",
    "allFiles=sorted(os.listdir(TEXT_DATA_DIR))\n",
    "all_docs_labels=[] \n",
    "for fname in allFiles:\n",
    "    fpath = os.path.join(TEXT_DATA_DIR, fname)\n",
    "    f = open(fpath,'rb')\n",
    "    all_sentences.append(f.read().decode('utf-8'))\n",
    "    base_name=basename(fname)\n",
    "    all_labels_index[base_name] = len(all_labels_index)\n",
    "    label=list(lbl for file,lbl in labels.items() if base_name.startswith(file))[0]\n",
    "    all_labels.append(label)\n",
    "    group=list(file for file,lbl in labels.items() if base_name.startswith(file))[0]\n",
    "    all_docs_labels.append(group)\n",
    "    f.close()\n",
    "    \n",
    "d = []\n",
    "for i in all_sentences:\n",
    "    words2 = text_to_word_sequence(i, lower=True, split=\" \")\n",
    "    d.append(words2)\n",
    "\n",
    "all_sentences = d\n",
    "\n",
    "for grp_Name in labels:\n",
    "    if grp_Name in done:\n",
    "        continue\n",
    "   \n",
    "    train_sentences = []  # list of text articles\n",
    "    train_labels = []  # list of label ids\n",
    "    test_sentences = []  # list of text articles\n",
    "    test_labels = []  # list of label ids\n",
    "    for j in range(0,len(all_docs_labels)):\n",
    "        if all_docs_labels[j]==grp_Name:\n",
    "            test_sentences.append(all_sentences[j])\n",
    "            test_labels.append(all_labels[j])\n",
    "        else:\n",
    "            train_sentences.append(all_sentences[j])\n",
    "            train_labels.append(all_labels[j])\n",
    "    if len(test_labels)==0:\n",
    "        continue\n",
    "    print (grp_Name)\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(i) for i in train_sentences)))\n",
    "    print(\"vocab len:%i\"%len(vocab))\n",
    "    # Reserve 0 for masking via pad_sequences\n",
    "    # Reserve 1 for unseen testing tokens\n",
    "    train_vocab_size = len(vocab) + 2\n",
    "    train_word_idx = dict((c, i + 2) for i, c in enumerate(vocab))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_vocabulary= train_word_idx\n",
    "    train_vocabulary_inv = vocab \n",
    "    train_vocabulary_inv.append(\"</PAD>\")\n",
    "    \n",
    "    print (\"Training: %i pos, %i neg, Testing: %i post, %i neg\"%(train_labels.count(1), train_labels.count(0),\n",
    "                                                                 test_labels.count(1), test_labels.count(0)))\n",
    "\n",
    "    itr_accuracy=[]\n",
    "    itr_pre_0=[]\n",
    "    itr_rec_0=[]\n",
    "    itr_f1_0=[]\n",
    "    itr_pre_1=[]\n",
    "    itr_rec_1=[]\n",
    "    itr_f1_1=[]    \n",
    "    \n",
    "    oldStart=0\n",
    "    \n",
    "    for itr in range(3, 4):# to get better and more robust results, run each fold 10 times\n",
    "      \n",
    "        if model_variation=='CNN-google': \n",
    "            model_name='GoogleNews-vectors-negative300.bin'\n",
    "            embedding_model = Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "            embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n",
    "                                                        for w in train_vocabulary_inv])]\n",
    "        elif model_variation=='CNN-RPC':\n",
    "            model_name='RPC.300'\n",
    "            embedding_model = Word2Vec.load(model_name)\n",
    "            embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n",
    "                                                        for w in train_vocabulary_inv])]\n",
    "        elif model_variation=='CNN-rand':\n",
    "            embedding_weights = None\n",
    "        else:\n",
    "            raise ValueError('Unknown model variation')    \n",
    "             \n",
    "        \n",
    "      \n",
    "    \n",
    "        \n",
    "        # Building model\n",
    "        # ==================================================\n",
    "        #\n",
    "        # graph subnet with one input and one output,\n",
    "        # convolutional layers concateneted in parallel\n",
    "        graph_in = Input(shape=(sequence_length, embedding_dim))\n",
    "        convs = []\n",
    "        for fsz in filter_sizes:\n",
    "            conv = Convolution1D(nb_filter=num_filters,\n",
    "                                 filter_length=fsz,\n",
    "                                 border_mode='valid',\n",
    "                                 activation='relu',\n",
    "                                 subsample_length=1)(graph_in)\n",
    "            pool = MaxPooling1D(pool_length=2)(conv)\n",
    "            flatten = Flatten()(pool)\n",
    "            convs.append(flatten)\n",
    "        \n",
    "        if len(filter_sizes)>1:\n",
    "            out = Merge(mode='concat')(convs)\n",
    "        else:\n",
    "            out = convs[0]\n",
    "        \n",
    "        graph = Model(input=graph_in, output=out)\n",
    "        \n",
    "        # main sequential model\n",
    "        model = Sequential()\n",
    "        if not model_variation=='CNN-static':\n",
    "            model.add(Embedding(len(train_vocabulary_inv)+1, embedding_dim, input_length=sequence_length,\n",
    "                                weights=embedding_weights))\n",
    "        \n",
    "        model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "        model.add(graph)\n",
    "        model.add(Dense(hidden_dims))\n",
    "        model.add(Dropout(dropout_prob[1]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        #\n",
    "        \n",
    "        # Training the model\n",
    "        X = []\n",
    "        for i in train_sentences:\n",
    "            x = [train_word_idx[w] for w in i]\n",
    "            X.append(x)\n",
    "        \n",
    "        X_train = pad_sequences(X,sequence_length)\n",
    "        \n",
    "        train_Y = np.asarray(train_labels) \n",
    "        model.fit(X_train, train_Y, batch_size=batch_size,nb_epoch=num_epochs, verbose=0 )#\n",
    "        \n",
    "        \n",
    " \n",
    "        X = []\n",
    "        for i in test_sentences:\n",
    "            x = [train_word_idx[w] if w in train_word_idx else 1 for w in i]\n",
    "            X.append(x)\n",
    "        \n",
    "        X_test = pad_sequences(X,sequence_length)\n",
    "        test_Y = test_labels\n",
    "        \n",
    "        test_count=len(X_test)\n",
    "        \n",
    "        probs = model.predict(X_test.reshape(test_count, -1))\n",
    "         \n",
    "        pred=(probs>0.5).astype(int).flatten() \n",
    "        correct=np.sum(pred==test_Y)\n",
    "        acc=correct/test_count\n",
    "        \n",
    "        CorrectClassifications=0\n",
    "        PosClassified=0\n",
    "        PosCorrectClassified=0\n",
    "        NegClassified=0\n",
    "        NegCorrectClassified=0 \n",
    "        #Calculate Accuracy \n",
    "        j=0\n",
    "        for  j in range(0,test_count) :\n",
    "            true_label=test_Y[j]\n",
    "            label=pred[j]    \n",
    "            if(label==true_label):\n",
    "                CorrectClassifications=CorrectClassifications+1\n",
    "            if(label==1):\n",
    "                PosClassified=PosClassified+1\n",
    "            else:\n",
    "                NegClassified=NegClassified+1\n",
    "            if(label==1 and label==true_label):\n",
    "                PosCorrectClassified=PosCorrectClassified+1 \n",
    "            if(label==0 and label==true_label):\n",
    "                NegCorrectClassified=NegCorrectClassified+1 \n",
    " \n",
    "        # precision, recall & F-Measure\n",
    "        TotalPos=test_Y.count(1)\n",
    "        TotalNeg=test_Y.count(0)\n",
    "      \n",
    "        if PosClassified==0:\n",
    "            posPre=0\n",
    "        else:\n",
    "            posPre=PosCorrectClassified/PosClassified\n",
    "        if TotalPos==0:\n",
    "            posRec=0\n",
    "        else:   \n",
    "            posRec=PosCorrectClassified/TotalPos\n",
    "        if (posPre+posRec)==0:\n",
    "            posF=0\n",
    "        else: \n",
    "            posF= 2*posPre*posRec/(posPre+posRec) \n",
    "        if NegClassified==0:\n",
    "            negPre=0\n",
    "        else: \n",
    "            negPre= NegCorrectClassified/NegClassified \n",
    "        if TotalNeg==0:\n",
    "            negRec=0\n",
    "        else: \n",
    "            negRec =NegCorrectClassified/TotalNeg\n",
    "        if (negPre+negRec)==0:\n",
    "            negF=0\n",
    "        else: \n",
    "            negF= 2*negPre*negRec/(negPre+negRec) \n",
    "        itr_accuracy.append(acc)\n",
    "        itr_pre_0.append(negPre)\n",
    "        itr_rec_0.append(negRec)\n",
    "        itr_f1_0.append(negF)\n",
    "        itr_pre_1.append(posPre)\n",
    "        itr_rec_1.append(posRec)\n",
    "        itr_f1_1.append(posF)\n",
    "        K.clear_session()\n",
    "        print(\"group %s itr %i, %i/%i, Accuracy:%f\"%(grp_Name, itr, CorrectClassifications,test_count,acc))\n",
    "        print(\"0 class (%i/%i/%i): precision:%f, recall:%f, fmeasure:%f\"%(NegClassified,NegCorrectClassified,TotalNeg,negPre,negRec,negF))\n",
    "        print(\"1 class (%i/%i/%i): precision:%f, recall:%f, fmeasure:%f\"%(PosClassified,PosCorrectClassified,TotalPos,posPre,posRec,posF))\n",
    "    \n",
    "    log_file = open(\"log_logocv.txt\",\"a\") \n",
    "    log_file.write(\"group %s\\n\"%grp_Name) \n",
    "    log_file.write(\"accuracy\\n\"+\" \".join(['%f' % num for num in itr_accuracy])+\"\\n\") \n",
    "    log_file.write(\"pre 0\\n\"+\" \".join(['%f' % num for num in itr_pre_0])+\"\\n\") \n",
    "    log_file.write(\"rec 0\\n\"+\" \".join(['%f' % num for num in itr_rec_0])+\"\\n\") \n",
    "    log_file.write(\"f1 0\\n\"+\" \".join(['%f' % num for num in itr_f1_0])+\"\\n\") \n",
    "    log_file.write(\"pre 1\\n\"+\" \".join(['%f' % num for num in itr_pre_1])+\"\\n\")  \n",
    "    log_file.write(\"rec 1\\n\"+\" \".join(['%f' % num for num in itr_rec_1])+\"\\n\")  \n",
    "    log_file.write(\"f1 1\\n\"+\" \".join(['%f' % num for num in itr_f1_1])+\"\\n\") \n",
    "    log_file.close()\n",
    "    \n",
    "    accuracy.append(np.average(itr_accuracy)) \n",
    "    pre_0.append(np.average(itr_pre_0))\n",
    "    rec_0.append(np.average(itr_rec_0))\n",
    "    f1_0.append(np.average(itr_f1_0))\n",
    "    pre_1.append(np.average(itr_pre_1))\n",
    "    rec_1.append(np.average(itr_rec_1))\n",
    "    f1_1.append(np.average(itr_f1_1))\n",
    "    \n",
    "    print(\"group %s: , Accuracy:%f\"%(grp_Name,np.average(itr_accuracy)))\n",
    "    print(\"0 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(itr_pre_0),np.average(itr_rec_0),np.average(itr_f1_0)))\n",
    "    print(\"1 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(itr_pre_1),np.average(itr_rec_1),np.average(itr_f1_1)))\n",
    "\n",
    "print(\"Average:\")  \n",
    "print(\"Accuracy:%f\"%(np.average(accuracy)))\n",
    "print(\"0 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(pre_0),np.average(rec_0),np.average(f1_0)))\n",
    "print(\"1 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(pre_1),np.average(rec_1),np.average(f1_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'وبالموت': 43726,\n",
       " 'والباغين': 41350,\n",
       " 'القرآني': 11848,\n",
       " 'تَقُولُونَ': 21914,\n",
       " 'ليقول': 34860,\n",
       " 'وتشرب،': 44382,\n",
       " 'طيبة': 27895,\n",
       " 'والتخلي': 41490,\n",
       " 'أبعدوا': 1815,\n",
       " 'تجنيد': 19634,\n",
       " 'المستغفرين،': 13212,\n",
       " 'يصاحب': 50996,\n",
       " 'توعز': 21759,\n",
       " 'ومواقعها': 48215,\n",
       " 'المقررات،': 13725,\n",
       " 'فأعطيت': 29308,\n",
       " 'المسيح': 13317,\n",
       " 'بغرائزه': 18089,\n",
       " 'عمّارها': 28781,\n",
       " 'فالماءُ': 29561,\n",
       " 'أَنْدَادًا': 3749,\n",
       " 'خيوطا': 24101,\n",
       " '\\xa0والتي': 1477,\n",
       " 'بثباته': 17167,\n",
       " 'الارض': 6954,\n",
       " 'جلدتكم': 22511,\n",
       " 'العظيم': 11215,\n",
       " 'المركزي': 13102,\n",
       " 'عفا': 28504,\n",
       " 'يتحركون': 49957,\n",
       " 'نَفْسِهِ': 39162,\n",
       " 'وتفسيق': 44569,\n",
       " 'منتصرين': 37627,\n",
       " 'مآلهم\\xa0': 35058,\n",
       " 'ومتسوليين': 47645,\n",
       " 'مجرى،': 35675,\n",
       " 'طقماً': 27807,\n",
       " 'المركز،': 13100,\n",
       " 'بمعركة': 18524,\n",
       " 'البخار': 7488,\n",
       " 'النقيب': 14380,\n",
       " 'سيدفعون': 26280,\n",
       " 'تحركات': 19723,\n",
       " 'يمكر': 51673,\n",
       " 'يحفظ': 50356,\n",
       " 'حاجة،': 22835,\n",
       " 'متفوق': 35495,\n",
       " 'يؤرق': 49805,\n",
       " 'الْجِنَّ': 14952,\n",
       " 'توحيدياً': 21719,\n",
       " 'اطارات': 5587,\n",
       " 'مربون': 36144,\n",
       " 'أجهزتك': 1984,\n",
       " 'بالتنفيذ': 16319,\n",
       " 'موطني': 37984,\n",
       " 'الخيال': 9255,\n",
       " 'سالك،': 25544,\n",
       " 'مهما': 37837,\n",
       " 'عمتي': 28704,\n",
       " 'يعملوا': 51249,\n",
       " 'و595': 39572,\n",
       " 'والمعسكرات': 43106,\n",
       " 'اسوء': 5463,\n",
       " 'أساء': 2369,\n",
       " 'يسنم\\r': 50896,\n",
       " 'مسلسلات،': 36528,\n",
       " 'هيومان': 39470,\n",
       " 'قانونية': 30831,\n",
       " 'ودحرت': 45469,\n",
       " 'لدول': 33067,\n",
       " 'ممارساتها\\xa0': 37505,\n",
       " 'ومسيحي،': 47897,\n",
       " 'والإضلال': 41055,\n",
       " 'استعمارها': 5261,\n",
       " 'بالإيذاء،': 16176,\n",
       " 'الإغاثية': 6676,\n",
       " 'مانعة': 35258,\n",
       " 'المقهورين': 13741,\n",
       " 'يرشد': 50640,\n",
       " 'ومقدمه': 48067,\n",
       " 'السابق،': 9856,\n",
       " 'وسيتم': 46020,\n",
       " 'ارجعوا': 5040,\n",
       " 'قلم': 31194,\n",
       " 'يعودوا': 51264,\n",
       " 'اخترعها': 4869,\n",
       " 'متعددة': 35467,\n",
       " 'نعم،': 38829,\n",
       " 'يهودية': 51930,\n",
       " 'أدواردو': 2215,\n",
       " 'اليمن؟،': 14813,\n",
       " 'mazrooq': 885,\n",
       " 'وقول': 47156,\n",
       " 'تحوي': 19831,\n",
       " 'بعمل': 18038,\n",
       " 'والتصدي': 41537,\n",
       " 'ألوان': 3037,\n",
       " 'رَجُلٌ': 25238,\n",
       " 'ومالحقها': 47606,\n",
       " 'حُميد': 23636,\n",
       " 'والتعاون،': 41560,\n",
       " 'غايه': 29099,\n",
       " 'وخشع': 45391,\n",
       " 'رياح': 25191,\n",
       " 'قالة': 30811,\n",
       " 'تحرك،': 19721,\n",
       " '1اي': 207,\n",
       " 'شكلياً،': 26859,\n",
       " 'وزراء': 45735,\n",
       " 'شهوة': 26935,\n",
       " 'المجاني،': 12724,\n",
       " 'يجاهدون': 50198,\n",
       " 'العلم': 11260,\n",
       " 'العناء': 11321,\n",
       " 'واهداف': 43570,\n",
       " 'وبوابتها': 43962,\n",
       " 'مَعْقِلِ': 38130,\n",
       " 'وإزهاق': 40155,\n",
       " 'تفضل': 21014,\n",
       " 'عَلَيْهِمْ': 28981,\n",
       " 'خذلانهم': 23792,\n",
       " 'التنفيذ': 8215,\n",
       " 'انتصاراتهم': 15247,\n",
       " 'الاسناد': 7086,\n",
       " 'النفطية': 14352,\n",
       " 'الفقراء': 11649,\n",
       " 'هشة': 39365,\n",
       " 'عضوية': 28465,\n",
       " 'الشريط': 10357,\n",
       " 'السائد': 9844,\n",
       " 'تهامة': 21615,\n",
       " 'أقتل': 2894,\n",
       " 'أموات': 3178,\n",
       " 'وضعفك،': 46347,\n",
       " 'الصفحة': 10617,\n",
       " 'يجتاز': 50205,\n",
       " 'للمخابرات': 34176,\n",
       " 'مقاربتها': 37174,\n",
       " 'clear': 611,\n",
       " 'الفوري': 11704,\n",
       " 'والممولة': 43177,\n",
       " 'فالتقى': 29494,\n",
       " 'يسيل': 50916,\n",
       " 'أصفار': 2571,\n",
       " 'داعش': 24183,\n",
       " 'الأمراض': 6355,\n",
       " 'مستوردون': 36478,\n",
       " 'والديزل': 42050,\n",
       " 'الممولة': 13872,\n",
       " 'يوحي': 51959,\n",
       " 'الأعماق': 6264,\n",
       " 'إفسادي': 4163,\n",
       " 'الأمر،': 6352,\n",
       " 'نصبر': 38708,\n",
       " 'النبوية': 14234,\n",
       " 'رَغَبٌ': 25259,\n",
       " 'الإِيمَانِ': 6783,\n",
       " 'سُلْطَانًا': 26474,\n",
       " 'الليل1': 12324,\n",
       " 'فاقه': 29456,\n",
       " 'قمّة': 31229,\n",
       " 'لمطاردة': 34463,\n",
       " 'بتقنيات': 17120,\n",
       " 'حبَشِيّاً،': 22961,\n",
       " 'قالوا\\xa0': 30817,\n",
       " 'إفريقية': 4160,\n",
       " 'الصادر': 10494,\n",
       " 'جزاءهم': 22442,\n",
       " 'الإصلاح،': 6642,\n",
       " 'والإخوان': 41019,\n",
       " 'same': 1039,\n",
       " 'والإفساد': 41070,\n",
       " 'أنتم': 3238,\n",
       " 'ننتهي': 39021,\n",
       " 'شَرّاً': 27009,\n",
       " 'ليعقد': 34839,\n",
       " 'غارقًا': 29071,\n",
       " 'خزرج': 23822,\n",
       " 'دلالات': 24383,\n",
       " 'بالعسكر': 16622,\n",
       " 'وبمقومات': 43926,\n",
       " 'والتفخيخ': 41594,\n",
       " 'شفافة': 26811,\n",
       " 'ورحب': 45643,\n",
       " 'يستحقون': 50752,\n",
       " 'ثورته\\xa0': 22174,\n",
       " 'موشايت': 37966,\n",
       " 'لقاءاته': 33397,\n",
       " 'بداهة،': 17432,\n",
       " 'الجنوبين': 8608,\n",
       " 'نظلم': 38795,\n",
       " 'معها،': 37051,\n",
       " 'حصاراً': 23238,\n",
       " '،لتتحوّل': 1580,\n",
       " 'العقائديين': 11222,\n",
       " 'التأملية،': 7711,\n",
       " 'كلفنا': 31924,\n",
       " 'تمرر': 21387,\n",
       " 'يداوى،': 50486,\n",
       " 'لاعتقالة': 32560,\n",
       " 'مترو': 35430,\n",
       " 'ومؤمنة': 47574,\n",
       " 'برسي': 17584,\n",
       " 'بآلياتهم': 15656,\n",
       " 'رهنوها': 25149,\n",
       " 'غدير': 29112,\n",
       " 'باتجاه': 15939,\n",
       " 'الوجة': 14598,\n",
       " 'ألحت': 2998,\n",
       " 'يستبعد': 50740,\n",
       " 'بالله؟': 16757,\n",
       " 'يمين': 51720,\n",
       " 'الغبية': 11398,\n",
       " 'وارتقيت': 40486,\n",
       " 'وسكنوا': 45926,\n",
       " 'مراقبون': 36130,\n",
       " 'الناطق': 14208,\n",
       " 'الجواسيس': 8641,\n",
       " 'المستطيرة': 13205,\n",
       " '\\xa0هنالك': 1467,\n",
       " 'خدّك': 23788,\n",
       " 'نجدها': 38475,\n",
       " '2005،': 229,\n",
       " 'الوسائل': 14658,\n",
       " 'وهبنة': 48479,\n",
       " 'وعدوان': 46572,\n",
       " 'أخضع': 2125,\n",
       " 'للأرض': 33529,\n",
       " 'العفنة': 11220,\n",
       " '\\xa0\\xa0والتي': 1214,\n",
       " 'بالقرار': 16699,\n",
       " 'بعملية': 18044,\n",
       " 'برقم': 17598,\n",
       " 'وبشجاعة': 43830,\n",
       " 'سرقوها': 25777,\n",
       " 'المزمعة': 13135,\n",
       " 'وقاص،': 46965,\n",
       " 'اموالكم': 15201,\n",
       " 'جئتنــا\\r\\r': 22195,\n",
       " 'رسمياً': 25025,\n",
       " 'جزءا': 22432,\n",
       " 'فارهة،': 29408,\n",
       " 'موضوع،': 37973,\n",
       " 'الحفظ،': 8896,\n",
       " 'فعال': 30026,\n",
       " 'middle': 898,\n",
       " 'تكلني؟': 21264,\n",
       " '180درجة': 115,\n",
       " 'عرفات': 28326,\n",
       " 'الارواح': 6959,\n",
       " 'المسافرين': 13173,\n",
       " 'وخسرانٌ': 45388,\n",
       " 'والتعايش': 41561,\n",
       " 'لنْ': 34608,\n",
       " 'border': 569,\n",
       " 'منقوصة': 37751,\n",
       " 'كذبا': 31739,\n",
       " 'كهذه': 32038,\n",
       " 'كذبتم،': 31744,\n",
       " 'بأصول': 15733,\n",
       " 'مضادة': 36758,\n",
       " 'ادعـى': 4958,\n",
       " 'بالمقتول': 16858,\n",
       " 'أَعْمَالَهُمْ﴾\\r': 3661,\n",
       " '1788م': 108,\n",
       " 'وتجعل': 44115,\n",
       " 'أرتموا': 2277,\n",
       " 'عصراً': 28443,\n",
       " 'بمكوناتهما': 18558,\n",
       " 'تعجبهم': 20713,\n",
       " 'للكرب': 34125,\n",
       " 'بدفعه': 17456,\n",
       " 'المفاجئات': 13628,\n",
       " 'المحيطات': 12893,\n",
       " 'توصيف\\xa0': 21741,\n",
       " 'هجرة': 39281,\n",
       " 'أشعة': 2511,\n",
       " 'والامريكيين': 41299,\n",
       " 'الأنفلات': 6431,\n",
       " 'توريث': 21726,\n",
       " 'مكافأة': 37287,\n",
       " 'للسياسي': 33946,\n",
       " 'مايتم': 35266,\n",
       " 'والجاهل': 41740,\n",
       " 'مكافحة': 37290,\n",
       " 'ميادين\\xa0': 38038,\n",
       " 'وحكومات': 45261,\n",
       " 'ردائه،': 24963,\n",
       " 'فوجد': 30361,\n",
       " 'وجهه': 45031,\n",
       " 'حروبه': 23155,\n",
       " 'نسمه': 38649,\n",
       " 'مخطط': 35959,\n",
       " 'برصاص': 17585,\n",
       " 'محضة': 35836,\n",
       " 'والأقليمي': 40944,\n",
       " 'القاسية': 11757,\n",
       " 'استفادة': 5276,\n",
       " 'بملاحم': 18559,\n",
       " 'وتحليله': 44185,\n",
       " 'وصناع': 46291,\n",
       " 'بأفراد': 15756,\n",
       " 'محسوماً،': 35831,\n",
       " 'يستمر': 50800,\n",
       " 'تَوَلَّى': 21941,\n",
       " '\\xa02011م': 1200,\n",
       " 'أجواف': 1990,\n",
       " 'haras': 758,\n",
       " 'بإتقان': 15838,\n",
       " 'احتفالا': 4785,\n",
       " 'سمّوا': 26045,\n",
       " 'الرَّسُولَ': 9753,\n",
       " 'التدهور': 7866,\n",
       " 'غاضبة': 29081,\n",
       " 'التواصل': 8240,\n",
       " 'historic': 771,\n",
       " 'قاصري': 30791,\n",
       " 'مزاعم': 36272,\n",
       " 'النوايا': 14418,\n",
       " 'يجدي': 50214,\n",
       " 'يعفو': 51223,\n",
       " 'شأنك': 26488,\n",
       " 'وجليل': 44981,\n",
       " 'يكتبون': 51527,\n",
       " 'الجمال': 8536,\n",
       " 'شق': 26822,\n",
       " 'مشاهد': 36587,\n",
       " 'إطلاق': 4098,\n",
       " 'متسع': 35438,\n",
       " 'يقترب': 51420,\n",
       " 'والغروب': 42561,\n",
       " 'انطلاق': 15363,\n",
       " 'الضرورية': 10756,\n",
       " 'تدعمون': 19939,\n",
       " 'فالمراهنون': 29564,\n",
       " 'كَالْأَنْعَامِ': 32131,\n",
       " 'والإيذاء': 41103,\n",
       " 'فوصلوا': 30374,\n",
       " 'رائد': 24778,\n",
       " 'يُرِيدُونَ': 52246,\n",
       " 'التكتم': 8140,\n",
       " 'الألغاز': 6327,\n",
       " 'التحديات': 7801,\n",
       " 'أستهداف': 2410,\n",
       " 'الشتات': 10283,\n",
       " 'النضالية': 14315,\n",
       " 'وانعتاقهم': 43539,\n",
       " 'ظلما': 27945,\n",
       " 'موقعها': 38007,\n",
       " 'ونفي،': 48416,\n",
       " 'يخفى': 50457,\n",
       " 'البائسين': 7431,\n",
       " 'الاخطاء': 6920,\n",
       " 'جنيف\\r': 22663,\n",
       " 'الشموس': 10431,\n",
       " 'ابناء': 4545,\n",
       " 'راهنتوا': 24847,\n",
       " 'بالوصاية': 16929,\n",
       " 'خلخلة': 23974,\n",
       " 'تداعت': 19904,\n",
       " 'توجهاتها': 21697,\n",
       " 'فاشله': 29433,\n",
       " 'ومنظمات': 48171,\n",
       " 'تحطيم': 19772,\n",
       " 'الوضعية': 14684,\n",
       " 'ننتهِ،': 39023,\n",
       " 'الاجرامية': 6851,\n",
       " 'يشحن': 50948,\n",
       " 'cities': 605,\n",
       " 'وتحرك': 44153,\n",
       " 'الأيتام': 6499,\n",
       " 'الغيل\\r': 11489,\n",
       " 'الحقوقيين': 8912,\n",
       " 'لتقويض': 32846,\n",
       " 'وتدفع': 44243,\n",
       " 'أمامنا': 3057,\n",
       " 'لِقَوْمٍ': 35008,\n",
       " 'فوج': 30357,\n",
       " 'بالهوية': 16921,\n",
       " 'حلاوة': 23408,\n",
       " 'وخلال': 45414,\n",
       " '35\\r': 325,\n",
       " 'بشعة': 17781,\n",
       " 'بحق\\xa0': 17311,\n",
       " 'سعوامريكية': 25824,\n",
       " 'تنام،': 21469,\n",
       " 'crime': 630,\n",
       " 'وانفضوا': 43547,\n",
       " 'بِشَيْءٍ': 19053,\n",
       " 'جاه،': 22273,\n",
       " 'بالبلهاء\\xa0': 16234,\n",
       " 'نملة،': 38998,\n",
       " 'اتصالات': 4631,\n",
       " 'الفرويدية': 11611,\n",
       " '2015': 258,\n",
       " 'أفعال،': 2850,\n",
       " 'برأس': 17527,\n",
       " 'أعدادًا': 2704,\n",
       " 'يتقدمون': 50102,\n",
       " 'المبنى': 12492,\n",
       " '٥٦': 52388,\n",
       " 'سلبي': 25937,\n",
       " 'التحسس': 7815,\n",
       " 'وحشي': 45203,\n",
       " 'يعود': 51263,\n",
       " 'والاخلاقية،': 41160,\n",
       " 'الخام': 9082,\n",
       " 'وهلاكك،': 48511,\n",
       " 'انقطاع': 15424,\n",
       " 'يسبتون': 50732,\n",
       " 'متاشرة': 35376,\n",
       " 'وقبلهما': 46985,\n",
       " 'متمنياً': 35529,\n",
       " 'بعزيز': 17990,\n",
       " 'التقليدية،': 8126,\n",
       " 'حضارة،': 23260,\n",
       " 'الطرفين': 10844,\n",
       " 'يراقبك،': 50582,\n",
       " 'والخور،': 41974,\n",
       " 'ورسوله،': 45667,\n",
       " 'للمرحلة': 34193,\n",
       " 'مفيدا،': 37160,\n",
       " '١': 52347,\n",
       " 'الاشراف\\r': 7098,\n",
       " 'وتشير': 44404,\n",
       " 'مؤيدي': 35127,\n",
       " 'الْبِرَّ': 14931,\n",
       " 'مستحقّات': 36379,\n",
       " 'التراجيديا': 7873,\n",
       " 'بمايحتاجونه': 18396,\n",
       " 'مدخل': 36008,\n",
       " 'غارة': 29064,\n",
       " 'والاستكبار': 41215,\n",
       " 'وانتهاء': 43494,\n",
       " 'والانتماء': 41311,\n",
       " 'حادثة\\xa0': 22850,\n",
       " 'اﻷسلحة': 15605,\n",
       " 'البيهقي': 7684,\n",
       " 'ندعو': 38550,\n",
       " 'رهيب': 25150,\n",
       " 'الجزئيات،': 8488,\n",
       " '1999': 206,\n",
       " 'الملاحقة': 13800,\n",
       " 'استقلال': 5305,\n",
       " 'ينقذهم': 51865,\n",
       " 'السنين،': 10125,\n",
       " 'وأخلص': 39737,\n",
       " 'انتخابات': 15231,\n",
       " 'قرآن': 30964,\n",
       " 'nahdeen': 916,\n",
       " 'وغير': 46814,\n",
       " 'الجذري': 8456,\n",
       " 'مستقرة': 36432,\n",
       " 'أنها': 3315,\n",
       " 'ضعفت': 27549,\n",
       " 'حياه': 23558,\n",
       " 'مَرُّوا': 38111,\n",
       " 'الْأَلْبَابِ': 14895,\n",
       " 'وطائراتهم': 46379,\n",
       " 'وأصهار،': 39865,\n",
       " 'نفعية،': 38896,\n",
       " 'ئل': 4491,\n",
       " 'وأفضل': 39942,\n",
       " 'الحادث': 8685,\n",
       " 'وبتعمد': 43746,\n",
       " 'كصمود': 31830,\n",
       " 'وبات': 43658,\n",
       " 'الدائري': 9276,\n",
       " 'كالغرباء': 31556,\n",
       " 'البلدان': 7624,\n",
       " 'وفرة': 46858,\n",
       " 'اشباعاً': 5489,\n",
       " 'مكتبه': 37323,\n",
       " 'شديد،': 26639,\n",
       " 'ثمرة،': 22141,\n",
       " 'أقتصر': 2892,\n",
       " 'مَعْشَرَ': 38129,\n",
       " '14': 63,\n",
       " 'أموالك،': 3183,\n",
       " 'عمارَة': 28695,\n",
       " 'يُقِيمُونَ': 52299,\n",
       " 'فلبسوا': 30190,\n",
       " 'بتفوقهم': 17112,\n",
       " 'واحداً،': 40410,\n",
       " 'الوتيرة': 14592,\n",
       " 'نذكر': 38556,\n",
       " 'اوروبيين': 15515,\n",
       " 'أجراء': 1945,\n",
       " 'يملكه': 51684,\n",
       " 'الأمنية': 6387,\n",
       " 'لابتزاز': 32454,\n",
       " 'رسخت': 25015,\n",
       " 'كلفة': 31921,\n",
       " 'يستخدم': 50758,\n",
       " 'بمعرفة': 18523,\n",
       " 'يشاركهم': 50925,\n",
       " 'لإيرادات': 32438,\n",
       " 'العادل': 10955,\n",
       " 'عشرين': 28416,\n",
       " 'تتوالى': 19522,\n",
       " 'الْبَغْيُ': 14925,\n",
       " 'توهموا': 21808,\n",
       " 'وان': 43466,\n",
       " 'بيته،': 18845,\n",
       " 'وَقَفَّيْنَا': 49592,\n",
       " '؟وهل': 1620,\n",
       " 'اعتمد': 5677,\n",
       " 'لإشراك': 32398,\n",
       " 'وَاحِدٍ': 49295,\n",
       " 'يذبحون': 50565,\n",
       " 'الباهظة': 7468,\n",
       " 'الآلآف': 5942,\n",
       " 'فشلها': 29958,\n",
       " 'كَمَنْ': 32180,\n",
       " 'مكان،': 37294,\n",
       " 'الزراعي': 9782,\n",
       " 'بالمجوسية،': 16788,\n",
       " 'التوازن': 8239,\n",
       " 'المقابر': 13677,\n",
       " 'طعاماً': 27782,\n",
       " 'والتنازل': 41655,\n",
       " 'لأصدقائك،': 32277,\n",
       " 'الاشهر،': 7102,\n",
       " 'العيون': 11360,\n",
       " 'حمم': 23485,\n",
       " 'يهينك؟': 51934,\n",
       " 'للجنوبين': 33796,\n",
       " 'تتجاوز،': 19360,\n",
       " 'يَخْتِلُ': 52062,\n",
       " 'الخاطئ،': 9075,\n",
       " 'safety': 1026,\n",
       " 'أبرزها': 1797,\n",
       " 'مذهل': 36092,\n",
       " 'بايدلوجية': 16992,\n",
       " 'بمساره': 18473,\n",
       " 'وزورا': 45767,\n",
       " 'ووو': 48685,\n",
       " 'تطالعون': 20581,\n",
       " 'ورفع': 45692,\n",
       " 'وثباتهم': 44833,\n",
       " 'injuries': 803,\n",
       " 'وشيوخ،': 46186,\n",
       " 'كلا،': 31903,\n",
       " 'أكله': 2982,\n",
       " 'يعلمها': 51236,\n",
       " 'محارمي': 35721,\n",
       " 'بقوة،': 18218,\n",
       " 'يحملنا': 50373,\n",
       " 'هيبة': 39456,\n",
       " 'ولمنطقة': 47496,\n",
       " 'ومشروع': 47917,\n",
       " 'المدثر': 12959,\n",
       " 'اجواء': 4760,\n",
       " 'حشد': 23224,\n",
       " 'نامل': 38356,\n",
       " 'السَّبِيلُ': 10216,\n",
       " 'السي': 10170,\n",
       " 'بالأمانة': 16127,\n",
       " 'باثنتي': 15946,\n",
       " 'و127': 39511,\n",
       " 'اساسيا': 5112,\n",
       " 'وجعلو': 44966,\n",
       " 'مِن': 38233,\n",
       " 'غيرها\\xa0\\xa0\\xa0': 29230,\n",
       " 'دقائق': 24354,\n",
       " 'تعلن،': 20808,\n",
       " 'الخمسه': 9235,\n",
       " 'ومنكر': 48183,\n",
       " 'ومداهمتهم': 47762,\n",
       " 'تصنيفا': 20507,\n",
       " 'الرئتان': 9544,\n",
       " 'والتطورات': 41552,\n",
       " 'يَرْجُونَ': 52086,\n",
       " 'خزينة': 23827,\n",
       " 'حرمات': 23144,\n",
       " 'استخدامها\\xa0': 5181,\n",
       " 'وَكَبِيرٍ': 49605,\n",
       " 'وحشيته': 45205,\n",
       " 'ينتهك': 51770,\n",
       " 'خلعوا': 23981,\n",
       " 'ومدارس': 47759,\n",
       " 'العظماء': 11211,\n",
       " 'سير': 26288,\n",
       " 'فبينما': 29657,\n",
       " '\\xa0والهدف': 1484,\n",
       " 'خصائص،': 23848,\n",
       " 'رِجَالٌ': 25276,\n",
       " 'وتخبطا': 44202,\n",
       " 'المحلات': 12871,\n",
       " 'وأقل': 39959,\n",
       " 'الرحم': 9628,\n",
       " 'المناطقة': 13891,\n",
       " 'وقصف': 47069,\n",
       " 'شأنُهُ': 26491,\n",
       " 'مهندس،': 37852,\n",
       " 'ارضاً': 5056,\n",
       " 'الموهوبين': 14123,\n",
       " 'الأخطاء': 6072,\n",
       " 'والغلواء': 42574,\n",
       " 'الأندلس': 6416,\n",
       " 'نخشى': 38526,\n",
       " 'مقالة': 37188,\n",
       " 'مداحش': 35995,\n",
       " 'الذَّكَرَ': 9526,\n",
       " 'ويجري': 48782,\n",
       " 'قواته': 31252,\n",
       " 'مثلكم': 35606,\n",
       " 'ولكل': 47442,\n",
       " 'الْمُجْرِمِينَ': 15076,\n",
       " 'بَعْدَ': 18937,\n",
       " 'محترفوا': 35769,\n",
       " 'المجال': 12718,\n",
       " 'توحي': 21715,\n",
       " 'والحقوقيين': 41886,\n",
       " 'أمنه': 3162,\n",
       " 'والعناية': 42527,\n",
       " 'نبوخذ': 38393,\n",
       " 'والمعلمين': 43112,\n",
       " 'صرحت': 27223,\n",
       " 'وإرصادا': 40150,\n",
       " '\\xa0لن': 1440,\n",
       " 'وريمة': 45727,\n",
       " 'technical': 1112,\n",
       " 'ساد': 25482,\n",
       " 'تسجد': 20283,\n",
       " 'وسلمت': 45952,\n",
       " 'باردة': 15985,\n",
       " 'كنوع': 32033,\n",
       " 'ونستيقظ': 48338,\n",
       " 'إنفاقه': 4337,\n",
       " 'ريالاً،': 25201,\n",
       " 'خَيْراً': 24136,\n",
       " 'جامعات،': 22251,\n",
       " 'وقامت': 46977,\n",
       " 'تخمتها': 19890,\n",
       " 'حياديته': 23556,\n",
       " 'للظالم': 34008,\n",
       " 'government': 734,\n",
       " 'ونتائجها': 48275,\n",
       " 'الوصف': 14676,\n",
       " 'الإجرام،': 6538,\n",
       " 'يتراجع': 49990,\n",
       " 'تتواصى': 19517,\n",
       " 'لَبِثُوا': 34924,\n",
       " 'المنخدعون': 13934,\n",
       " 'قياسي': 31334,\n",
       " 'الغيث': 11485,\n",
       " 'تلمص': 21335,\n",
       " 'جيولوجيا،': 22779,\n",
       " 'والدولي،': 42045,\n",
       " 'وتضمر': 44452,\n",
       " 'سفارتها': 25851,\n",
       " 'وزوجة': 45760,\n",
       " 'done': 667,\n",
       " 'ومعلن': 48014,\n",
       " 'العم': 11282,\n",
       " 'لخرج': 33032,\n",
       " 'للعملية': 34049,\n",
       " 'تنجو': 21505,\n",
       " 'الخميس': 9238,\n",
       " 'المستكبرة': 13230,\n",
       " 'الشجاعة': 10284,\n",
       " 'بكبيركم': 18253,\n",
       " 'الأبد؟': 5974,\n",
       " 'الفترة،': 11546,\n",
       " 'للتاريخ': 33692,\n",
       " 'المتحدة”': 12529,\n",
       " 'ولأولياء': 47319,\n",
       " 'سمومه': 26038,\n",
       " 'شاملاً،': 26559,\n",
       " 'الشرعي': 10328,\n",
       " 'النعوة': 14336,\n",
       " 'ترعرعت': 20096,\n",
       " 'يكرهه': 51549,\n",
       " 'وتكفيريي': 44642,\n",
       " 'والوسائل': 43379,\n",
       " 'صواريخها': 27385,\n",
       " 'كثرة': 31684,\n",
       " 'إعداء': 4118,\n",
       " 'يَدَيْهِ': 52072,\n",
       " 'الاستبدادية': 6980,\n",
       " 'وبصبغة': 43835,\n",
       " 'اللقطة': 12293,\n",
       " 'الوافدين': 14580,\n",
       " 'ينفذ': 51839,\n",
       " 'لتصرفاتهم': 32785,\n",
       " 'الخطير\\xa0': 9180,\n",
       " 'أزعج': 2354,\n",
       " 'يتورع': 50164,\n",
       " 'الْقَانِطِينَ،': 15017,\n",
       " 'صدقة،': 27188,\n",
       " 'مَا': 38084,\n",
       " 'بفئة': 18120,\n",
       " 'يرددونها': 50629,\n",
       " 'فيريدون': 30437,\n",
       " 'انك': 15432,\n",
       " 'ويتم': 48762,\n",
       " 'وبالأحرى': 43688,\n",
       " 'واستنكرت': 40609,\n",
       " 'ومخيمات': 47756,\n",
       " 'نقاط': 38917,\n",
       " 'وهزيمة': 48500,\n",
       " 'المتعة': 12605,\n",
       " 'البصري،': 7567,\n",
       " 'تباعاً': 19255,\n",
       " 'جَلِّهِمْ': 22798,\n",
       " 'عندك،': 28797,\n",
       " 'تتساقط': 19418,\n",
       " 'الغرب': 11415,\n",
       " 'اهتماماته': 15472,\n",
       " 'ومسئوليها': 47837,\n",
       " 'للشهادة': 33967,\n",
       " 'al': 497,\n",
       " 'ونظافة': 48377,\n",
       " 'التاريخ': 7724,\n",
       " 'وعروبةً': 46599,\n",
       " 'الحجاج': 8754,\n",
       " 'المتردي؟': 12552,\n",
       " 'باعلانها': 16058,\n",
       " 'نراه': 38560,\n",
       " 'اتهامه': 4652,\n",
       " 'جعلتهم': 22478,\n",
       " 'ركاب': 25119,\n",
       " '\\xa0ومن': 1509,\n",
       " 'الْجَهْرَ': 14946,\n",
       " 'المشاركون': 13333,\n",
       " 'الفرحون': 11581,\n",
       " 'فَنَتَّبِعَ': 30691,\n",
       " 'قطعي': 31139,\n",
       " 'الرسول': 9662,\n",
       " 'جراء': 22377,\n",
       " 'خبيث،': 23754,\n",
       " 'فأقرر': 29312,\n",
       " 'قليلاً': 31209,\n",
       " 'يطوروا': 51120,\n",
       " 'سراحهم': 25758,\n",
       " 'والابتهاج': 41113,\n",
       " 'إضعافها': 4089,\n",
       " 'هزالة': 39350,\n",
       " 'وأحتلها': 39692,\n",
       " 'واستطاع': 40547,\n",
       " 'هَبَاءً': 39476,\n",
       " 'مدة': 36005,\n",
       " 'يُؤمّن': 52194,\n",
       " 'قمعه': 31225,\n",
       " 'وحول': 45305,\n",
       " 'وأصبحت': 39855,\n",
       " 'التعبيرية،': 8012,\n",
       " 'يدلك': 50532,\n",
       " 'والمقرات': 43143,\n",
       " 'والصمت': 42337,\n",
       " 'استمرارية': 5332,\n",
       " 'مالايحتمل': 35229,\n",
       " 'مختبر': 35925,\n",
       " 'الصاق': 10511,\n",
       " 'احتقار': 4788,\n",
       " 'بآلة': 15655,\n",
       " 'ونظامه': 48380,\n",
       " 'خططها': 23919,\n",
       " 'مشوب': 36653,\n",
       " 'أهنأ': 3378,\n",
       " 'تثقيفي': 19541,\n",
       " 'مسئوليتها': 36318,\n",
       " 'وإضعافه': 40185,\n",
       " 'بأمراء': 15785,\n",
       " 'انتقال': 15262,\n",
       " 'ونساء': 48321,\n",
       " 'واكد': 40836,\n",
       " 'سابقيه': 25472,\n",
       " 'الطَّيْرِ': 10904,\n",
       " 'المصنع': 13422,\n",
       " 'يسعى': 50856,\n",
       " 'بعلبة': 18030,\n",
       " 'القمع': 11964,\n",
       " 'ثالثاً،': 22037,\n",
       " 'الجياشة': 8661,\n",
       " 'لاهدافهم': 32608,\n",
       " 'بالزيدية': 16484,\n",
       " 'القاني': 11782,\n",
       " 'عَنْهُ': 29007,\n",
       " 'وأنزل': 40031,\n",
       " 'الشَّمْسِ': 10486,\n",
       " 'يلتقطها': 51591,\n",
       " 'تُحمّل': 21954,\n",
       " 'المغلوطة': 13623,\n",
       " 'ينقص': 51868,\n",
       " 'رأوا': 24733,\n",
       " 'وقدموها': 47037,\n",
       " 'آبه': 1629,\n",
       " 'يبين': 49894,\n",
       " 'جِنَّةٍ': 22826,\n",
       " 'بالعراق': 16618,\n",
       " 'التجسس': 7781,\n",
       " 'الخمسة': 9233,\n",
       " 'ووجهائها': 48595,\n",
       " 'والجامعة': 41738,\n",
       " 'آَخَرَ': 1751,\n",
       " 'الخطر،': 9164,\n",
       " 'السلعة': 10067,\n",
       " '–اجمالا': 52410,\n",
       " 'قاسيه': 30789,\n",
       " 'قطر\\r': 31122,\n",
       " 'وحلفائها': 45273,\n",
       " 'شمولي،': 26890,\n",
       " 'صواريخنا،': 27383,\n",
       " 'مشروع': 36610,\n",
       " 'وقف': 47103,\n",
       " 'ذُكر': 24699,\n",
       " 'وسيفشلون': 46047,\n",
       " 'أَفْنَاهُ': 3676,\n",
       " 'الْمُسَخَّرِ': 15081,\n",
       " 'فالاحتمالات': 29481,\n",
       " 'القلوب': 11950,\n",
       " 'تامة': 19243,\n",
       " 'يباركها': 49839,\n",
       " 'شن': 26892,\n",
       " 'قَبْلُ': 31371,\n",
       " 'الثانوية،': 8319,\n",
       " 'نفكر': 38900,\n",
       " 'والمنازل': 43179,\n",
       " 'تعالى؟': 20665,\n",
       " 'بالمزرق': 16812,\n",
       " 'الهزلية': 14507,\n",
       " 'وسيحاسب': 46022,\n",
       " 'وَمِنْ': 49666,\n",
       " 'ويواسيهم': 49183,\n",
       " 'وتعاظمها': 44475,\n",
       " 'ويرومون': 48881,\n",
       " 'أبوها': 1853,\n",
       " 'لصوت': 33246,\n",
       " 'الأولاد': 6483,\n",
       " '90': 458,\n",
       " 'المتألق': 12497,\n",
       " '،لا': 1578,\n",
       " 'مَتَّعْنَاهُ': 38100,\n",
       " 'بعضهم': 18008,\n",
       " 'الجرح': 8472,\n",
       " 'والمحدثين\\xa0': 42895,\n",
       " 'ويستفزون': 48908,\n",
       " 'واللقاءات': 42768,\n",
       " 'الميدان': 14135,\n",
       " 'الالباب': 7230,\n",
       " 'أَمْوَالَكُمْ': 3720,\n",
       " 'الاقتصادية\\xa0': 7202,\n",
       " 'حظوظه': 23293,\n",
       " 'كمعلم': 31997,\n",
       " 'للخارطة': 33853,\n",
       " 'السنتها': 10113,\n",
       " 'أثريين': 1926,\n",
       " 'الإساءة': 6587,\n",
       " 'واسياده': 40652,\n",
       " 'والخدمة': 41940,\n",
       " 'هاجسا': 39226,\n",
       " 'حراء': 23067,\n",
       " 'الوهاب': 14749,\n",
       " 'الدستوري': 9341,\n",
       " 'والموالي': 43242,\n",
       " 'السابقتين،': 9859,\n",
       " 'وليّ': 47549,\n",
       " 'اهميتة': 15492,\n",
       " 'وتجبرهم': 44106,\n",
       " 'سيطالها': 26321,\n",
       " 'السراجيمنذ': 9962,\n",
       " 'الفاسدة': 11520,\n",
       " 'فايعفشل': 29626,\n",
       " 'المخططات': 12938,\n",
       " 'وسلمية': 45956,\n",
       " 'الشاب': 10243,\n",
       " '\\xa0كما': 1420,\n",
       " 'وقدرة': 47026,\n",
       " 'وقوتها': 47144,\n",
       " 'فورا': 30366,\n",
       " 'مأسور': 35069,\n",
       " 'يحرم': 50333,\n",
       " 'أوحال': 3407,\n",
       " 'الاسرائيلي\\xa0': 7066,\n",
       " 'قَوْمَهُ': 31412,\n",
       " 'ويخرجونها': 48827,\n",
       " 'حربها': 23094,\n",
       " 'وتزيد': 44317,\n",
       " 'حافة': 22882,\n",
       " 'الدهر': 9397,\n",
       " 'وقتهم،': 47015,\n",
       " 'ليحموا': 34768,\n",
       " 'وجلست': 44977,\n",
       " 'اختار': 4859,\n",
       " 'وشخص': 46087,\n",
       " 'صحة،': 27142,\n",
       " 'مراسلات': 36118,\n",
       " 'عذرية': 28297,\n",
       " 'والتكنولوجي': 41635,\n",
       " 'والجبروت': 41745,\n",
       " 'وتناثرت': 44698,\n",
       " 'المراهنة': 13055,\n",
       " 'صنفان': 27360,\n",
       " 'استطاعو': 5237,\n",
       " 'محجبة': 35784,\n",
       " 'إمكانيتك': 4285,\n",
       " 'عمان': 28699,\n",
       " 'الخسارة': 9133,\n",
       " 'مرحلته': 36189,\n",
       " '142': 66,\n",
       " 'بأسابيع': 15705,\n",
       " 'قضوا': 31106,\n",
       " 'عمالتهم': 28698,\n",
       " 'السعوديون': 10004,\n",
       " 'يخدم': 50421,\n",
       " 'للمسلحين': 34207,\n",
       " 'gun': 742,\n",
       " 'كانَ': 31595,\n",
       " 'فكلاهما': 30173,\n",
       " 'أجمعت': 1969,\n",
       " 'بختان\\xa0تكشفت': 17366,\n",
       " 'بالمئات\\xa0': 16766,\n",
       " 'يتحمل': 49963,\n",
       " 'قرارا': 30998,\n",
       " 'الإجازات': 6534,\n",
       " 'ليتكلم': 34744,\n",
       " 'برفضها': 17594,\n",
       " 'و76': 39583,\n",
       " 'هِلالٌ': 39502,\n",
       " 'القبض': 11793,\n",
       " 'البُنى': 7689,\n",
       " 'عمار': 28693,\n",
       " 'بالأشعة': 16114,\n",
       " 'وسكينة': 45928,\n",
       " 'الفتنوية': 11554,\n",
       " 'أرداه': 2286,\n",
       " 'يطاق،': 51091,\n",
       " 'القاعدة': 11764,\n",
       " 'أَنفُسَهُمْ': 3728,\n",
       " 'شارف': 26529,\n",
       " 'people': 957,\n",
       " 'القريبة': 11885,\n",
       " 'ينصره': 51820,\n",
       " 'وجميع': 44994,\n",
       " 'from': 721,\n",
       " 'طِيبِ': 27922,\n",
       " 'رأيناك': 24745,\n",
       " 'عنس،': 28813,\n",
       " 'كحركة': 31711,\n",
       " 'الحركية': 8820,\n",
       " 'بقضاء': 18198,\n",
       " 'متعهدي': 35481,\n",
       " 'أَحَدٍ': 3586,\n",
       " 'إمكانياتهم': 4283,\n",
       " 'التعويل': 8043,\n",
       " 'الانشقاق': 7344,\n",
       " 'حاضنة': 22881,\n",
       " 'المجاورة': 12730,\n",
       " 'يمارسون': 51632,\n",
       " 'والليالي': 42774,\n",
       " 'والقاسية': 42637,\n",
       " 'الوسيله': 14671,\n",
       " 'فقتربنا': 30095,\n",
       " 'هاتفها': 39220,\n",
       " 'مادية،': 35172,\n",
       " 'ارتكبتها': 5030,\n",
       " 'مصاف': 36684,\n",
       " 'تطبقها': 20584,\n",
       " 'تجوب': 19639,\n",
       " 'نتناول': 38442,\n",
       " 'ينكر': 51873,\n",
       " 'القضاء\\r\\r': 11908,\n",
       " 'الأمة\\r': 6346,\n",
       " 'الخرافات': 9118,\n",
       " 'والعمر': 42513,\n",
       " 'أَعْمَلُ': 3663,\n",
       " 'كونه': 32082,\n",
       " 'عصفت': 28448,\n",
       " 'السعودية،': 10002,\n",
       " 'شاغــل': 26543,\n",
       " 'استعبدوه': 5249,\n",
       " 'أناطها': 3218,\n",
       " 'تحتاط': 19670,\n",
       " 'سيارة': 26186,\n",
       " 'لكلمة': 33479,\n",
       " 'فرجت': 29835,\n",
       " 'للامم': 33649,\n",
       " 'الْمَرْأَةُ': 15052,\n",
       " '١٥٨': 52360,\n",
       " 'سواك': 26127,\n",
       " 'ولديه': 47402,\n",
       " 'يفترقون': 51337,\n",
       " 'أبية': 1861,\n",
       " 'ضمير': 27587,\n",
       " 'التفريق': 8089,\n",
       " 'قضيتهم': 31113,\n",
       " 'دعوته': 24320,\n",
       " 'نرفض': 38566,\n",
       " 'إهليلجي': 4368,\n",
       " 'صفقوا': 27284,\n",
       " 'والواضح': 43363,\n",
       " 'وتحريض': 44157,\n",
       " 'مَسْجِدَهُ': 38118,\n",
       " 'وصلاة،': 46268,\n",
       " 'لمحت': 34388,\n",
       " 'وجرحاهم': 44928,\n",
       " 'زاره،': 25297,\n",
       " 'أستهدف': 2411,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hezbollah\n",
      "vocab len:52509\n",
      "Training: 380 pos, 1230 neg, Testing: 678 post, 0 neg\n",
      "Epoch 1/4\n",
      "1610/1610 [==============================] - 245s - loss: 0.3165 - acc: 0.8857     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/4\n",
      "1610/1610 [==============================] - 843s - loss: 0.0953 - acc: 0.9739     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/4\n",
      "1610/1610 [==============================] - 225s - loss: 0.0545 - acc: 0.9901     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/4\n",
      "1610/1610 [==============================] - 241s - loss: 0.0522 - acc: 0.9851     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "group hezbollah itr 3, 678/678, Accuracy:1.000000\n",
      "0 class (0/0/0): precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class (678/678/678): precision:1.000000, recall:1.000000, fmeasure:1.000000\n",
      "group hezbollah: , Accuracy:1.000000\n",
      "0 class: precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class: precision:1.000000, recall:1.000000, fmeasure:1.000000\n",
      "Average:\n",
      "Accuracy:1.000000\n",
      "0 class: precision:0.000000, recall:0.000000, fmeasure:0.000000\n",
      "1 class: precision:1.000000, recall:1.000000, fmeasure:1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    " \n",
    "import os\n",
    "from os.path import basename,isfile\n",
    "import csv  \n",
    "from keras import backend as K\n",
    "from functools import reduce\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "#\n",
    "# Model Variations. See Kim Yoon’s Convolutional Neural Networks for \n",
    "# Sentence Classification, Section 3 for detail.\n",
    "sequence_length = 1000\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 8\n",
    "filter_sizes = (3, 4)\n",
    "num_filters = 128\n",
    "#dropout_prob = (0.1, 0.75)\n",
    "dropout_prob = (0.1, 0.35)\n",
    "hidden_dims = 256 \n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 4\n",
    "#val_split = 0.33\n",
    "\n",
    "done=['sermons', 'houthis']\n",
    "#done=['ACLU','AEU','alqaeda','AndrewMurray','aqis','asl','ast',\n",
    "#        'Bahai','boko-haram','DorothyDay','Ghandi','YV','isis',\n",
    "#        'JohnPiper','LiberalJudaism','MalcolmX','MehrBaba','NaumanKhan',\n",
    "#        'nusra-front','PastorAnderson','PeterGomes','Rabbinic',\n",
    "#        'Schizophrenia','SeaShepherds','Shepherd','Stalin','taliban',\n",
    "#        'Unabomber','Unitarian','WBC','hamza-yusuf','yasir-qadhi',\n",
    "#        'zaki-naik','suhaib-webb','kkk','white-supremacis']\n",
    "TEXT_DATA_DIR='arabic-docs'\n",
    "accuracy=[]\n",
    "pre_0=[]\n",
    "rec_0=[]\n",
    "f1_0=[]\n",
    "pre_1=[]\n",
    "rec_1=[]\n",
    "f1_1=[]\n",
    "model_variation = 'CNN-rand'  #  CNN-rand | CNN-google | CNN-RPC\n",
    "#print('Model variation is %s' % model_variation)\n",
    "\n",
    "# Data Preparatopn\n",
    "# ==================================================\n",
    "#\n",
    "# Load data\n",
    "#print(\"Loading data...\")\n",
    " \n",
    "\n",
    "#load labels\n",
    "filePath='arabic-groups-labels.txt'\n",
    "labels={}\n",
    "with open(filePath,'r') as intputFile:\n",
    "        reader=csv.reader(intputFile,delimiter=',')\n",
    "        for fname,y in reader:\n",
    "            labels[fname]=int(y)\n",
    "# load all data\n",
    "all_sentences = []  # list of text articles\n",
    "all_labels_index = {}  # dictionary mapping label name to numeric id\n",
    "all_labels = []  # list of label ids\n",
    "\n",
    "allFiles=sorted(os.listdir(TEXT_DATA_DIR))\n",
    "all_docs_labels=[] \n",
    "for fname in allFiles:\n",
    "    fpath = os.path.join(TEXT_DATA_DIR, fname)\n",
    "    f = open(fpath,'rb')\n",
    "    all_sentences.append(f.read().decode('utf-8'))\n",
    "    base_name=basename(fname)\n",
    "    all_labels_index[base_name] = len(all_labels_index)\n",
    "    label=list(lbl for file,lbl in labels.items() if base_name.startswith(file))[0]\n",
    "    all_labels.append(label)\n",
    "    group=list(file for file,lbl in labels.items() if base_name.startswith(file))[0]\n",
    "    all_docs_labels.append(group)\n",
    "    f.close()\n",
    "    \n",
    "d = []\n",
    "for i in all_sentences:\n",
    "    words2 = text_to_word_sequence(i, lower=True, split=\" \")\n",
    "    d.append(words2)\n",
    "\n",
    "all_sentences = d\n",
    "\n",
    "for grp_Name in labels:\n",
    "    if grp_Name in done:\n",
    "        continue\n",
    "   \n",
    "    train_sentences = []  # list of text articles\n",
    "    train_labels = []  # list of label ids\n",
    "    test_sentences = []  # list of text articles\n",
    "    test_labels = []  # list of label ids\n",
    "    for j in range(0,len(all_docs_labels)):\n",
    "        if all_docs_labels[j]==grp_Name:\n",
    "            test_sentences.append(all_sentences[j])\n",
    "            test_labels.append(all_labels[j])\n",
    "        else:\n",
    "            train_sentences.append(all_sentences[j])\n",
    "            train_labels.append(all_labels[j])\n",
    "    if len(test_labels)==0:\n",
    "        continue\n",
    "    print (grp_Name)\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(i) for i in train_sentences)))\n",
    "    print(\"vocab len:%i\"%len(vocab))\n",
    "    # Reserve 0 for masking via pad_sequences\n",
    "    # Reserve 1 for unseen testing tokens\n",
    "    train_vocab_size = len(vocab) + 2\n",
    "    train_word_idx = dict((c, i + 2) for i, c in enumerate(vocab))\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_vocabulary= train_word_idx\n",
    "    train_vocabulary_inv = vocab \n",
    "    train_vocabulary_inv.append(\"</PAD>\")\n",
    "    \n",
    "    print (\"Training: %i pos, %i neg, Testing: %i post, %i neg\"%(train_labels.count(1), train_labels.count(0),\n",
    "                                                                 test_labels.count(1), test_labels.count(0)))\n",
    "\n",
    "    itr_accuracy=[]\n",
    "    itr_pre_0=[]\n",
    "    itr_rec_0=[]\n",
    "    itr_f1_0=[]\n",
    "    itr_pre_1=[]\n",
    "    itr_rec_1=[]\n",
    "    itr_f1_1=[]    \n",
    "    \n",
    "    oldStart=0\n",
    "    \n",
    "    for itr in range(3, 4):# to get better and more robust results, run each fold 10 times\n",
    "      \n",
    "        if model_variation=='CNN-google': \n",
    "            model_name='GoogleNews-vectors-negative300.bin'\n",
    "            embedding_model = Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "            embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n",
    "                                                        for w in train_vocabulary_inv])]\n",
    "        elif model_variation=='CNN-RPC':\n",
    "            model_name='RPC.300'\n",
    "            embedding_model = Word2Vec.load(model_name)\n",
    "            embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n",
    "                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n",
    "                                                        for w in train_vocabulary_inv])]\n",
    "        elif model_variation=='CNN-rand':\n",
    "            embedding_weights = None\n",
    "        else:\n",
    "            raise ValueError('Unknown model variation')    \n",
    "             \n",
    "        \n",
    "      \n",
    "    \n",
    "        \n",
    "        # Building model\n",
    "        # ==================================================\n",
    "        #\n",
    "        # graph subnet with one input and one output,\n",
    "        # convolutional layers concateneted in parallel\n",
    "         \n",
    "        # main sequential model\n",
    "        model = Sequential()\n",
    "        if not model_variation=='CNN-static':\n",
    "            model.add(Embedding(len(train_vocabulary_inv)+1, embedding_dim, input_length=sequence_length,\n",
    "                                weights=embedding_weights))\n",
    " \n",
    " \n",
    "        model.add(Bidirectional(LSTM(64)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # try using different optimizers and different optimizer configs\n",
    "        model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    " \n",
    "        #model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n",
    "        #model.add(graph)\n",
    "        #model.add(Dense(hidden_dims))\n",
    "        #model.add(Dropout(dropout_prob[1]))\n",
    "        #model.add(Activation('relu'))\n",
    "        #model.add(Dense(1))\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        #model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        #model.add(Activation('sigmoid'))\n",
    "        #\n",
    "        \n",
    "        # Training the model\n",
    "        X = []\n",
    "        for i in train_sentences:\n",
    "            x = [train_word_idx[w] for w in i]\n",
    "            X.append(x)\n",
    "        \n",
    "        X_train = pad_sequences(X,sequence_length)\n",
    "        \n",
    "        train_Y = np.asarray(train_labels) \n",
    "        model.fit(X_train, train_Y, batch_size=batch_size,nb_epoch=num_epochs, verbose=1 )#\n",
    "        \n",
    "        \n",
    " \n",
    "        X = []\n",
    "        for i in test_sentences:\n",
    "            x = [train_word_idx[w] if w in train_word_idx else 1 for w in i]\n",
    "            X.append(x)\n",
    "        \n",
    "        X_test = pad_sequences(X,sequence_length)\n",
    "        test_Y = test_labels\n",
    "        \n",
    "        test_count=len(X_test)\n",
    "        \n",
    "        probs = model.predict(X_test.reshape(test_count, -1))\n",
    "         \n",
    "        pred=(probs>0.5).astype(int).flatten() \n",
    "        correct=np.sum(pred==test_Y)\n",
    "        acc=correct/test_count\n",
    "        \n",
    "        CorrectClassifications=0\n",
    "        PosClassified=0\n",
    "        PosCorrectClassified=0\n",
    "        NegClassified=0\n",
    "        NegCorrectClassified=0 \n",
    "        #Calculate Accuracy \n",
    "        j=0\n",
    "        for  j in range(0,test_count) :\n",
    "            true_label=test_Y[j]\n",
    "            label=pred[j]    \n",
    "            if(label==true_label):\n",
    "                CorrectClassifications=CorrectClassifications+1\n",
    "            if(label==1):\n",
    "                PosClassified=PosClassified+1\n",
    "            else:\n",
    "                NegClassified=NegClassified+1\n",
    "            if(label==1 and label==true_label):\n",
    "                PosCorrectClassified=PosCorrectClassified+1 \n",
    "            if(label==0 and label==true_label):\n",
    "                NegCorrectClassified=NegCorrectClassified+1 \n",
    " \n",
    "        # precision, recall & F-Measure\n",
    "        TotalPos=test_Y.count(1)\n",
    "        TotalNeg=test_Y.count(0)\n",
    "      \n",
    "        if PosClassified==0:\n",
    "            posPre=0\n",
    "        else:\n",
    "            posPre=PosCorrectClassified/PosClassified\n",
    "        if TotalPos==0:\n",
    "            posRec=0\n",
    "        else:   \n",
    "            posRec=PosCorrectClassified/TotalPos\n",
    "        if (posPre+posRec)==0:\n",
    "            posF=0\n",
    "        else: \n",
    "            posF= 2*posPre*posRec/(posPre+posRec) \n",
    "        if NegClassified==0:\n",
    "            negPre=0\n",
    "        else: \n",
    "            negPre= NegCorrectClassified/NegClassified \n",
    "        if TotalNeg==0:\n",
    "            negRec=0\n",
    "        else: \n",
    "            negRec =NegCorrectClassified/TotalNeg\n",
    "        if (negPre+negRec)==0:\n",
    "            negF=0\n",
    "        else: \n",
    "            negF= 2*negPre*negRec/(negPre+negRec) \n",
    "        itr_accuracy.append(acc)\n",
    "        itr_pre_0.append(negPre)\n",
    "        itr_rec_0.append(negRec)\n",
    "        itr_f1_0.append(negF)\n",
    "        itr_pre_1.append(posPre)\n",
    "        itr_rec_1.append(posRec)\n",
    "        itr_f1_1.append(posF)\n",
    "        K.clear_session()\n",
    "        print(\"group %s itr %i, %i/%i, Accuracy:%f\"%(grp_Name, itr, CorrectClassifications,test_count,acc))\n",
    "        print(\"0 class (%i/%i/%i): precision:%f, recall:%f, fmeasure:%f\"%(NegClassified,NegCorrectClassified,TotalNeg,negPre,negRec,negF))\n",
    "        print(\"1 class (%i/%i/%i): precision:%f, recall:%f, fmeasure:%f\"%(PosClassified,PosCorrectClassified,TotalPos,posPre,posRec,posF))\n",
    "    \n",
    "    log_file = open(\"log_logocv.txt\",\"a\") \n",
    "    log_file.write(\"group %s\\n\"%grp_Name) \n",
    "    log_file.write(\"accuracy\\n\"+\" \".join(['%f' % num for num in itr_accuracy])+\"\\n\") \n",
    "    log_file.write(\"pre 0\\n\"+\" \".join(['%f' % num for num in itr_pre_0])+\"\\n\") \n",
    "    log_file.write(\"rec 0\\n\"+\" \".join(['%f' % num for num in itr_rec_0])+\"\\n\") \n",
    "    log_file.write(\"f1 0\\n\"+\" \".join(['%f' % num for num in itr_f1_0])+\"\\n\") \n",
    "    log_file.write(\"pre 1\\n\"+\" \".join(['%f' % num for num in itr_pre_1])+\"\\n\")  \n",
    "    log_file.write(\"rec 1\\n\"+\" \".join(['%f' % num for num in itr_rec_1])+\"\\n\")  \n",
    "    log_file.write(\"f1 1\\n\"+\" \".join(['%f' % num for num in itr_f1_1])+\"\\n\") \n",
    "    log_file.close()\n",
    "    \n",
    "    accuracy.append(np.average(itr_accuracy)) \n",
    "    pre_0.append(np.average(itr_pre_0))\n",
    "    rec_0.append(np.average(itr_rec_0))\n",
    "    f1_0.append(np.average(itr_f1_0))\n",
    "    pre_1.append(np.average(itr_pre_1))\n",
    "    rec_1.append(np.average(itr_rec_1))\n",
    "    f1_1.append(np.average(itr_f1_1))\n",
    "    \n",
    "    print(\"group %s: , Accuracy:%f\"%(grp_Name,np.average(itr_accuracy)))\n",
    "    print(\"0 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(itr_pre_0),np.average(itr_rec_0),np.average(itr_f1_0)))\n",
    "    print(\"1 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(itr_pre_1),np.average(itr_rec_1),np.average(itr_f1_1)))\n",
    "\n",
    "print(\"Average:\")  \n",
    "print(\"Accuracy:%f\"%(np.average(accuracy)))\n",
    "print(\"0 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(pre_0),np.average(rec_0),np.average(f1_0)))\n",
    "print(\"1 class: precision:%f, recall:%f, fmeasure:%f\"%(np.average(pre_1),np.average(rec_1),np.average(f1_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
